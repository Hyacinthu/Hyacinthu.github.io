---
title: 李宏毅机器学习笔记-7-Training Tip 1：Batch Normalization
date: 2022-05-12 09:00:00 +0800
categories: [机器学习入门]
tags: [李宏毅机器学习课程笔记]
pin: false
author: 郑威琦

toc: true
comments: true
math: false
mermaid: true

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# （一）Batch Normalization（批次标准化）

&emsp;&emsp;这个方法就是上节课（笔记5）所留下的如何能铲平山的一个方法。

## 1、Batch Normalization的最终目标

&emsp;&emsp;其最终达到的效果是将error surface处理地较为平整，能够更好地也更快地收敛。

## 2、Batch Normalization的具体过程

### （1）在训练（training）当中

&emsp;&emsp;Batch Normalization其实就是对一个训练批次（batch）中所有的特征（feature）进行归一化/标准化（Normalization）。先是求得所有数据的均值，当然这个数据是个向量，对于不同数据中同一维度的数加起来除以batch size得到均值，然后再得到标准差，然后根据每一项数据减去相应的均值后，再除以标准差来作为归一化后的数据。如图：

![feature normalization](/assets/blog_res/2022-05-12-Thursday.assets/Feature%20Normalization.png){:hight="75%" width="75%"}

&emsp;&emsp;输入层的话，由于没有激活函数，所以直接对原始数据进行处理就好了。那么当进入到第二层之后，对每一层的输入都要进行归一化，但由于第二层之后每一层都有激活函数，那么此时既可以再激活之前进行归一化，也可以再激活之后进行归一化。对于sigmoid来说在激活之前进行归一化会稍稍好一些。如图：

![deep feature normalization](/assets/blog_res/2022-05-12-Thursday.assets/deep%20feature%20normalization.png){:hight="75%" width="75%"}

&emsp;&emsp;此时就是牵一发而动全身。

### （2）在测试（testing/inference）当中

&emsp;&emsp;在做实验的时候也许是有比较成块的测试集，这时候是可以将测试集也像训练集一样分成batch。但是在实际中，往往都是来一条数据就要输入到训练好的网络中来输出，所以此时的均值$\mu$和标准差$\sigma$该如何取？如下图：

![testing](/assets/blog_res/2022-05-12-Thursday.assets/testing.png){:hight="75%" width="75%"}

&emsp;&emsp;在pytorch中会在训练的时候计算一个叫做动态平均（moving average）的数，他会动态的计算并更新$\bar{\mu}$，那么对于标准差而言也是一样的。其中$p$为一个超参数。然后把计算出来的均值和标准差用于测试时候的归一化。

## 3、其他的归一化方法

&emsp;&emsp;当然归一化的方法不是唯一的，除了批次归一化还有很多的归一化方法。如图：

![solutions](/assets/blog_res/2022-05-12-Thursday.assets/solutions.png){:hight="75%" width="75%"}

# 附录

[【知乎】什么是批标准化 (Batch Normalization)](https://zhuanlan.zhihu.com/p/24810318)

[【csdn】什么是批标准化 (Batch Normalization)](https://blog.csdn.net/hffhjh111/article/details/86994445)

&emsp;&emsp;当然上面俩是野生的资料，更多的资料还可以通过论文、老师的youtube等深入了解。