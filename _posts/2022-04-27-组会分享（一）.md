---
title: 组会分享（一）
date: 2022-04-27 09:00:00 +0800
categories: [组会]
tags: [组会个人分享]
pin: false
author: 郑威琦

toc: true
comments: true
math: false
mermaid: true

---

## 1 前沿科技视野拓展

[小蛮驴“沦陷”事件](https://www.bilibili.com/video/BV1b3411K7f1?spm_id_from=333.880.my_history.page.click)

[CVPR2022（IEEE国际计算机视觉与模式识别会议）](https://zhuanlan.zhihu.com/p/478648512)

[机器之心](https://www.jiqizhixin.com/articles/2022-04-19-3)

量子位

## 2 个人博客搭建

[1分钟免费博客搭建](https://www.bilibili.com/video/BV14S4y1N7Yr?spm_id_from=333.999.0.0)

## 3 神经网络反向误差传播

### **3.1 神经网络**

&emsp;&emsp;神经网络实质是在生物学基础上（人类神经元）建立起来的数学模型。

![M-P神经元](/assets/blog_res/2022-04-27-Thirsday.assets/M-P神经元.jpg)

&emsp;&emsp;多个神经元同向排列（纵或横）组成一层（layer）。

![神经网络](/assets/blog_res/2022-04-27-Thirsday.assets/神经网络.jpg)

### **3.2 感知机**

&emsp;&emsp;感知机（Perceptron）由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层为M-P神经元。

![感知机](/assets/blog_res/2022-04-27-Thirsday.assets/Perceptron.jpg)

![感知机计算](/assets/blog_res/2022-04-27-Thirsday.assets/感知机计算0.jpg)

&emsp;&emsp;感知机学习旨在求出将训练数据进行线性划分的分离超平面。  
&emsp;&emsp;感知机的参数更新规则是基于梯度下降的，在这里不多作详述，直接给出西瓜书所给更新规则。

![感知机神经网络更新规则](/assets/blog_res/2022-04-27-Thirsday.assets/感知机神经网络更新规则.jpg)

### **3.3 基于标准梯度下降的误差逆传播算法**

&emsp;&emsp;假设有网络N，其输入层有d个神经元，隐藏层有q个M-P神经元，输出层有l个神经元，且输入层与隐藏层、隐藏层与输出层为全连接。设输入层第i个神经元到隐藏层第h个神经元的权值为ν<sub>ih</sub>，隐藏层第h个神经元的输入为α<sub>h</sub>，阈值为γ<sub>h</sub>，输出为b<sub>h</sub>，隐藏层第h个神经元到输出层第j个神经元的权值为ω<sub>hj</sub>，输出层第j个神经元的输入为β<sub>j</sub>，阈值为θ<sub>j</sub>，输出为y<sub>j</sub>尖。

![net](/assets/blog_res/2022-04-27-Thirsday.assets/net0.jpg)

&emsp;&emsp;并且，令激活函数为Sigmoid函数。

![Sigmoid](/assets/blog_res/2022-04-27-Thirsday.assets/Sigmoid0.jpg)

&emsp;&emsp;由此，根据前向传播规则，我们可计算出这些参数的值：

![cal](/assets/blog_res/2022-04-27-Thirsday.assets/cal%20nums0.jpg)

&emsp;&emsp;然后，由于将网络在(X<sub>k</sub>, Y<sub>k</sub>)上的均方误差视为损失函数，则将损失函数最小化就是我们参数更新的依据，故计算出目标的负梯度方向，并以此对参数进行调整。

&emsp;&emsp;均方误差如下：

![均方误差](/assets/blog_res/2022-04-27-Thirsday.assets/E.jpg)

&emsp;&emsp;根据梯度的数学定义，计算每个参数对应的负梯度。如下：

![前两个参数](/assets/blog_res/2022-04-27-Thirsday.assets/前两个参数0.jpg)

![后两个参数](/assets/blog_res/2022-04-27-Thirsday.assets/后两个参数0.jpg)

&emsp;&emsp;总而言之：

![总之](/assets/blog_res/2022-04-27-Thirsday.assets/总之.jpg)  
![g](/assets/blog_res/2022-04-27-Thirsday.assets/g.jpg)  
![e](/assets/blog_res/2022-04-27-Thirsday.assets/eh.jpg)

&emsp;&emsp;最后给出算法流程：

![算法流程](/assets/blog_res/2022-04-27-Thirsday.assets/算法流程.jpg)

## 4 附录

### **4.1 感知机代码**

>
from math import *
from Activation_Function import *

# 解决线性可分的基于梯度下降的感知机
class Perceptron:
    def __init__(self, input, output_true, learning_rate):
        self.x = input
        self.w = [0 for i in range(len(input[0]))] # 权值
        self.y = [0 for i in range(len(input))] # y尖
        self.y_true = output_true    # y
        self.bias = 0
        self.lr = learning_rate  # 学习率
        self.w_changes = [0 for i in range(len(input[0]))]

    # 前向传播计算出y值
    def Forward_Propagation(self):
        for i in range(len(self.x)):
            for j in range(len(self.x[i])):
                self.y[i] = self.y[i] + self.x[i][j] * self.w[j]
            self.y[i] = self.y[i] + self.bias
            self.y[i] = Sgn(self.y[i])

    # 基于梯度下降的权值更新，通过不断地极小化目标函数（欧式距离/均方差）
    def Weight_Update(self, i):
        for j in range(len(self.w)):
            self.w_changes[j] = self.lr * self.y_true[i] * self.x[i][j]
            self.w[j] = self.w[j] + self.w_changes[j]
        self.bias = self.bias + self.y_true[i] * self.lr

    # 判断是否正确完成线性分割
    def whether_all_correct(self):
        for i in range(len(self.y)):
            if self.y[i] * self.y_true[i] <= 0:
                return i
            continue
        return -1

def train():
    input = [[1, 1], [1, 0], [0, 1], [0, 0]]
    output_true = [1, -1, -1, -1]
    # input = [[3, 3], [4, 3], [1, 1]]
    # output_true = [1, 1, -1]
    learning_rate = 0.5
    p = Perceptron(input, output_true, learning_rate)
    p.Forward_Propagation()
    i = p.whether_all_correct()
    while i != -1:
        print("x: " + str(p.x) + " y: " + str(p.y) + " w:　" + str(p.w) + " b: " + str(p.bias))
        p.Weight_Update(i)
        p.Forward_Propagation()
        i = p.whether_all_correct()
    print("x: " + str(p.x) + " y: " + str(p.y) + " w:　" + str(p.w) + " b: " + str(p.bias))
>

### **4.2 BP代码（有问题，但可以帮助理解算法流程）**

>
from math import *
from Activation_Function import *

# 构建神经网络
class NeuralNetwork:
    def __init__(self, input_neural_num, hidden_neural_num, output_neural_num, X, learning_rate):
        # 网络层数
        self.input_neural_num = input_neural_num    # 输入层神经元个数
        self.hidden_neural_num = hidden_neural_num  # 隐藏层神经元个数
        self.output_neural_num = output_neural_num  # 输出层神经元个数
        # 输入
        self.x = X  # For instance(Page 105 in 西瓜书): X = [[0, 0, 1], [1, 0, 0], [0, 2, 0], [1, 1, 0], [2, 2, 0]]
        self.lr = learning_rate  # 学习率
        # 输出
        self.niu_ih = [0 for i in range(input_neural_num * hidden_neural_num)]  # input -> hidden
        self.gamma = [0 for i in range(hidden_neural_num)]  # 隐藏层阈值向量
        self.omega_hj = [0 for i in range(hidden_neural_num * output_neural_num)]  # hidden -> output
        self.theta = [0 for i in range(output_neural_num)]  # 输出层阈值向量
        self.y = [0 for i in range(output_neural_num)]
        # 参数
        self.b = [0 for i in range(hidden_neural_num)]
        # 梯度项
        self.g = [0 for i in range(output_neural_num)]    # 输出层梯度项
        self.e = [0 for i in range(hidden_neural_num)]    # 隐藏层梯度项

    def ForwardPropagation(self, x):
        alpha = []  # 隐藏层输入
        alpha_h = 0
        # 计算隐藏层输出向量b
        for h in range(self.hidden_neural_num):
            for i in range(self.input_neural_num):
                alpha_h += x[i] * self.niu_ih[i * self.hidden_neural_num + h]
            alpha.append(alpha_h)
            alpha_h = 0
            self.b[h] = Sigmoid(alpha[h] - self.gamma[h])
        beta = []   # 输出层输入
        beta_j = 0
        # 计算输出层输出向量y
        for j in range(self.output_neural_num):
            for h in range(self.hidden_neural_num):
                beta_j += self.b[h] * self.omega_hj[h * self.output_neural_num + j]
            beta.append(beta_j)
            beta_j = 0
            self.y[j] = Sigmoid(beta[j] - self.theta[j])

    def Cal_g(self, x):
        for j in range(self.output_neural_num):
            self.g[j] = self.y[j] * (1 - self.y[j]) * (x[self.input_neural_num] - self.y[j])

    def Cal_e(self):
        for h in range(self.hidden_neural_num):
            contribution = 0
            for j in range(self.output_neural_num):
                contribution += self.g[j] * self.omega_hj[h * self.output_neural_num + j]
            self.e[h] = self.b[h] * (1 - self.b[h]) * contribution

    def BP(self, x):
        for h in range(self.hidden_neural_num):
            for j in range(self.output_neural_num):
                self.omega_hj[h * self.output_neural_num + j] += self.lr[1] * self.g[j] * self.b[h]
        for j in range(self.output_neural_num):
            self.theta[j] -= self.lr[1] * self.g[j]
        for i in range(self.input_neural_num):
            for h in range(self.hidden_neural_num):
                self.niu_ih[i * self.hidden_neural_num + h] += self.lr[0] * self.e[h] * x[i]
        for h in range(self.hidden_neural_num):
            self.gamma[h] -= self.lr[0] * self.e[h]

    # 迭代
    def run(self):
        for x in self.x:
            self.ForwardPropagation(x)
            self.Cal_g(x)
            self.Cal_e()
            self.BP(x)

if __name__ == "__main__":
    input_neural_num = 2
    hidden_neural_num = 2
    output_neural_num = 2
    X = [[0, 0, 1], [1, 0, 0], [1, 1, 0], [0, 2, 0], [2, 2, 0]]
    learning_rate = [0.6, 0.4]
    nn = NeuralNetwork(input_neural_num, hidden_neural_num, output_neural_num, X, learning_rate)
    for i in range(200):
        print("y: " + str(nn.y))
        nn.run()
>