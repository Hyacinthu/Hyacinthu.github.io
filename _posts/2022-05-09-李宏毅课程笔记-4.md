---
title: 李宏毅机器学习笔记-4-批次与动量
date: 2022-05-09 14:00:00 +0800
categories: [机器学习入门]
tags: [李宏毅机器学习课程笔记]
pin: false
author: 郑威琦

toc: true
comments: true
math: false
mermaid: true

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# （一）一些英文术语

![batch](/assets/blog_res/2022-05-10-Tuesday.assets/batch.png){:hight="50%" width="50%"}

## 1、batch（批次）

&emsp;&emsp;将一个训练集进行切分，每一块就叫一个batch，训练过程中只有输入一个batch中的所有数据才可以计算出损失函数进而更新参数。

&emsp;&emsp;个人理解：一个batch中的数据数相当于输入神经元的个数，即所构造的神经网络的输入层神经元个数。

## 2、update（更新）

&emsp;&emsp;一次参数更新。

## 3、epoch

&emsp;&emsp;对训练集切分出的所有块都训练过一次叫做一次epoch（机翻意思是时期、纪元）。

## 4、shuffle（洗牌）

&emsp;&emsp;由于对训练集可以做出不同形式的切分，在完成一次epoch后对训练集重新切分叫做shuffle，机翻就很形象了：洗牌。

# （二）浅谈batch（也是一个超参数Parameter）

## 1、Small batch v.s. Large batch

&emsp;&emsp;Small batch的意思是切分出的batch数目小，Large batch的意思是切分出的batch数目大，比如说20条数据，每一条数据都分为一个batch。那么根据课程上所给出的实验数据可以看出，切分出来的batch数目越小，更新一次的时间会较长一些，但是更加powerful；而切分的batch数目越多，就越noisy。

![batch character](/assets/blog_res/2022-05-10-Tuesday.assets/batch%20character.png){:hight="50%" width="50%"}

&emsp;&emsp;但是结合更新一次和完成所有切分处理训练的时间来看，切分的batch数目不那么大，反而会更快。

![v](/assets/blog_res/2022-05-10-Tuesday.assets/v.png){:hight="50%" width="50%"}

&emsp;&emsp;然而batch数目适当多一些，其优势往往会比单纯追求训练速度要大很多。当batch数目相对较多时，其训练的准确率比较少的batch数目的要好很多，收敛地越好。究其原因在于，当batch增多时，前一个是critical point的，对应着下一个batch可能就不是critical point，这样就会继续下降，而不是卡在一个较高的点位。这也为什么说batch数目越多，越noisy，而越是“吵闹”，就越有可能跳出当前的临界点。

![accuracy](/assets/blog_res/2022-05-10-Tuesday.assets/accuracy.png){:hight="50%" width="50%"}

&emsp;&emsp;以下是二者的一个总结对比：

![batch vs](/assets/blog_res/2022-05-10-Tuesday.assets/batch%20vs.png){:hight="50%" width="50%"}

## 2、好的Minima（Flat Minima）与坏的Minima（Sharp Minima）

![Flat Sharp](/assets/blog_res/2022-05-10-Tuesday.assets/Flat%20Sharp.png){:hight="50%" width="50%"}

&emsp;&emsp;有时候小的batch会造成过拟合，即在训练集上表现良好，但是在测试集上的表现却让人失望。而大的batch在一定程度上会减少这样的落差。  

&emsp;&emsp;这个现象的产生来自于上面的Flat Minima与Sharp Minima。好的Minima比较平缓，坏的则是个陡峭的峡谷，假设测试集背后的分布恰好是训练集分布的平移，且恰好高峰与低谷相对应，那么此时在训练集表现很好的模型到了测试集上就奇差无比。为什么说这样的低谷不好呢？就在于此。你看Flat Minima，地势平缓，即便平移很多，测试集上的结果也与训练集上的相差不多。  

&emsp;&emsp;那么当batch比较小的时候，noisy的程度不是那么高，就不容易跳出这个低谷，而只有batch相对较大时才能跳出。

# （三）动量（Momentum）

&emsp;&emsp;将梯度下降更新参数的过程很形象地描绘出来就是以下的样子：

![gradient descent](/assets/blog_res/2022-05-10-Tuesday.assets/gradient%20descent.png){:hight="50%" width="50%"}

&emsp;&emsp;那么被动量改进后的梯度下降在找到梯度后，不立即以负梯度进行更新，而是把新的负梯度与上一个行进的方向向量进行向量叠加，得出的这个叠加向量作为更新方向，这就让梯度下降具有了一定的“惯性”。

&emsp;&emsp;个人理解：这里应该是借用物理学当中的动量的概念，来给梯度下降进行优化的，使之也具有了动量，或者说惯性。这样的好处就是对于一些较为平缓的Minima也有机会跳出去，因为到了低谷的点并不会立即停止，而是依照其惯性继续照相应方向行进。

![corperation](/assets/blog_res/2022-05-10-Tuesday.assets/corperation.png){:hight="50%" width="50%"}

# （四）关于这节课程的疑问

&emsp;&emsp;1、batch在实际的神经网络中如何操作的？涉及到实际的操作和代码是怎样的？目前对于batch等的认知还比较抽象，需要在实践中进行完善。

&emsp;&emsp;2、batch作为超参数在实际过程中如何很好地进行调节？