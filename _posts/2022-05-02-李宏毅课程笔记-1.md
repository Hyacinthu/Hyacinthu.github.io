---
title: 李宏毅机器学习笔记-1-机器学习入门
date: 2022-05-02 14:00:00 +0800
categories: [机器学习入门]
tags: [李宏毅机器学习课程笔记]
pin: false
author: 郑威琦

toc: true
comments: true
math: false
mermaid: true

---

# （一） 什么是机器学习

## Machine Learning ≈ Looking for Function

&emsp;&emsp;

# （二） 如何使机器学习

## 1、步骤一：找一个函数（Function with Unknown Parameters）

&emsp;&emsp;y = b + wx<sub>1</sub>——模型（Model），x——特征（Feature），w——权重（weight），b——偏置（bias）

## 2、步骤二：定义损失函数（Define Loss from Training Data）

&emsp;&emsp;选择什么样的损失函数？是均值？亦或是均方差？

## 3、步骤三：通过参数更新找一个使得当前损失函数值最小的模型（Optimization）

（1）w\*，b\* = arg min(w，b) *L*  

（2）如何去更新参数呢？-> 梯度下降？  

（3）缺陷  
&emsp;&emsp;提出一个问题：Local minima 与 Global minima，即局部最优与全局最优的问题，单纯的梯度下降容易陷入到局部最优。那如何解决呢？在后面的课程中有讲解。

## 4、小结

&emsp;&emsp;总而言之，上述提到的三个步骤合起来就是训练（Trainging）。

![Traning](/assets/blog_res/2022-05-02-Monday.assets/Trainging.png)  

<center>ML Framework</center>

&emsp;&emsp;

# （三）对于模型的简单深化（纵向优化）

## 1、增加特征（输入）个数

&emsp;&emsp;这里以课上的视频播放量为例，如果说wx<sub>1</sub>是将前1天的播放量作为输入预测下一天的，那么从表面上看显然不是很合理，那么就可以将前七天的播放量作为输入，于是就变为：
$$y = b + \sum \limits_ {i=i}^ 7 w_ j x_ j$$

&emsp;&emsp;类似的，可以将前二十八天的播放量作为输入：

$$y = b + \sum \limits_ { i = i } ^ { 28 } w_ j x_ j$$

&emsp;&emsp;同理，前五十六天：
$$y = b + \sum \limits_ { i = i } ^ { 56 } w_ j x_ j$$

&emsp;&emsp;但是，从老师所给出的数据中可以看出：二十八天与五十六天的损失函数值已经相差无几，近乎没有进一步减小损失。

## 2、深化为神经网络（基于数学的基础上借助生物学将其表达形象化后的形式）

&emsp;&emsp;

# （四）对于机器学习过程中寻找合适函数的剖析

## 1、线性模型实在是太粗糙了，我们需要模拟得更加真切的模型！

<img src = "../assets/blog_res/2022-05-02-Monday.assets/Linear%20models.png" width = "50%">

&emsp;&emsp;

## 2、假设有如下曲线需要机器学习

![curve](/assets/blog_res/2022-05-02-Monday.assets/curve.png)

&emsp;&emsp;那么，可以将其拆分成y = constant和一些折线型的函数之和：

![red curve](/assets/blog_res/2022-05-02-Monday.assets/red%20curve.png)

&emsp;&emsp;于是乎，既有了个可用的既定结论（已经在数学中被证明）：  

*<center>所有分段线性曲线均可被表示为常数线与一些折线的和</center>*

&emsp;&emsp;

## 3、那对于连续可导的函数如何分解呢？

&emsp;&emsp;取有限的点并连线，取得点可少可多，越多就越逼近原曲线，可以一定程度上理解为微元法。

## 4、如何用数学表示折线形函数呢？

&emsp;&emsp;可以利用带有参数的Sigmoid函数近似去表示。

![sigmoid with paramters](/assets/blog_res/2022-05-02-Monday.assets/Sigmoid%20Function%20with%20Parameters.png)

&emsp;&emsp;那么对于新模型，则有如下表达式：

$$y = b + \sum \limits_ i c_ i Sigmoid(b_ i + w_ i x_ 1)$$

&emsp;&emsp;即如图：

![new model](/assets/blog_res/2022-05-02-Monday.assets/new%20model%20within%20Sigmoid.png)

&emsp;&emsp;于是乎，对于*y = b + $\sum \limits_j$w<sub>j</sub>x<sub>j</sub>*，就有：

$$y = b + \sum \limits_ i c_ i Sigmoid(b_ i + \sum \limits_ j w_ {ij} x_ j)$$

## 5、小结

&emsp;&emsp;综上，借助生物学中神经网络的形象和概念，就可以将以上深化过程形象地描绘成以下：

![deepen1](/assets/blog_res/2022-05-02-Monday.assets/deepen1.png)  

&emsp;&emsp;那么借助于线性代数的知识可以将其进一步简化为：

![linear math form](/assets/blog_res/2022-05-02-Monday.assets/Lenear%20Math%20form.png)  

&emsp;&emsp;同理：

![deepen2](/assets/blog_res/2022-05-02-Monday.assets/deepen2.png)

&emsp;&emsp;则，其中需要更新的参数就有如下这些：

![parameters](/assets/blog_res/2022-05-02-Monday.assets/parameters.png)

&emsp;&emsp;

# （五）Sigmoid --> Relu

## 1、Relu（Rectified Linear Unit）可以更好模拟hard sigmoid

&emsp;&emsp;两个Relu叠起来就是一个hard sigmoid。

$$y = b + \sum \limits_ {2i} c_ i max(0, b_ i + \sum \limits_ j w_ {ij} x_ j)$$
  
## 2、那为什么Relu函数比较好呢？

&emsp;&emsp;后面的课程会讲到

&emsp;&emsp;

# （六）深度学习

## &emsp;&emsp;经过上面的铺垫，那么就可以自然而然地抛出神经网络与深度网络和深度学习的概念与模型了。

![Deep Learning](/assets/blog_res/2022-05-02-Monday.assets/Deep%20Learning.png)

## 1、常见的深度学习模型

&emsp;&emsp;AlexNet(2012, 8 layers, los_rate: 16.4%), VGG(2014, 19 layers, los_rate: 7.3%), GoogleNet(2014, 22 layers, los_rate: 6.7%)

![deep network](/assets/blog_res/2022-05-02-Monday.assets/deep%20network.png)

## 2、“深”的意义何在呢？多层隐藏层实际上在哪里做出了怎样的贡献呢？

&emsp;&emsp;

# （七）本节课提出的待解决的问题

## 1、全局最优与局部最优

## 2、Relu函数为什么比Sigmoid好？

## 3、多层隐藏层的作用