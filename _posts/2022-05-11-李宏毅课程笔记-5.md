---
title: 李宏毅机器学习笔记-5-做好Optimization之学习率
date: 2022-05-11 09:00:00 +0800
categories: [机器学习入门]
tags: [李宏毅机器学习课程笔记]
pin: false
author: 郑威琦

toc: true
comments: true
math: false
mermaid: true

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# （一）学习率

## 1、学习率优化一：可以灵活应对不同的error surface

&emsp;&emsp;学习率这一超参数在训练的过程中也是发挥着举重若轻的作用，简单来说，学习率决定了根据梯度下降更新参数的时候参数下降的速率，或是每次更新的步伐大小。如果学习率偏大，在应对较陡的参数曲线时会引起震荡，但是在应对较为平缓的参数曲线时则可以发挥出其快速接近的作用。

![lr](/assets/blog_res/2022-05-11-Wednesday.assets/lr.png){:hight="75%" width="75%"}

&emsp;&emsp;而对于不同的参数，其对应的参数曲线不一致，所以对于不同的参数也要有不同的d学习率。我们希望在面对较为陡峭的error surface时，学习率可以小一点；在面对较为平缓的error surface时，学习率可以大一点。

![dpndlr](/assets/blog_res/2022-05-11-Wednesday.assets/different%20parameters%20%20needs%20different%20lr.png){:hight="75%" width="75%"}

&emsp;&emsp;那么对任意一个参数有：

$$\theta^{t+1}_i \leftarrow \theta^{t}_i - \eta g^{t}_i$$

$$g^{t}_i = \frac{\partial L}{\partial \theta_i}|_{\theta=\theta_i}$$

&emsp;&emsp;其中，t为迭代序号，i为参数序号，$\eta$为学习率。在此基础上对参数更新进行优化：

$$\theta^{t+1}_i \leftarrow \theta^{t}_i - \frac{\eta}{\sigma^{t}_i}g^t_i$$

&emsp;&emsp;其中增加$\sigma$这一参数来根据error surface动态调整学习率。同时给出$\sigma^t_i$：

$$\sigma^t_i = \sqrt{\frac{1}{t+1}\displaystyle \sum^t_{i=0} (g^t_i)^2}$$

&emsp;&emsp;以下给出具体迭代次数的$\sigma$：

![sigma](/assets/blog_res/2022-05-11-Wednesday.assets/sigma.png){:hight="75%" width="75%"}

&emsp;&emsp;从图中可以看出，$\sigma^t_i$就为过去所有更新的梯度平方的均值再开更号。  

&emsp;&emsp;那么具体为何这么设计？这么设计又是怎样使得学习率动态调整呢？以下图为例：

![root mean square](/assets/blog_res/2022-05-11-Wednesday.assets/root%20mean%20square.png){:hight="75%" width="75%"}

&emsp;&emsp;如果$\theta_1$对应的曲线较缓，那么上面每一次计算的梯度就较小，求得他们的均方根就比较小，每一次就会进行较大的调整。同理对应$\theta_2$的曲线。

## 2、学习率优化二：可以应对同一参数的error surface中的陡峭程度不同部分

![same error surface](/assets/blog_res/2022-05-11-Wednesday.assets/same%20error%20surface.png){:hight="75%" width="75%"}

&emsp;&emsp;为了能够应对同一参数曲线的不同陡峭部分，加入了新的参数$\alpha(0<\alpha<1)$来控制之前迭代的梯度与当前迭代的梯度的占比，即重要性。

$$\theta^{t+1}_i \leftarrow \theta^{t}_i - \frac{\eta}{\sigma^{t}_i}g^t_i$$

$$\sigma^t_i = \sqrt{\alpha(\sigma^{t-1}_i)^2 + (1-\alpha)(g^t_i)^2}$$

![RMSProp1](/assets/blog_res/2022-05-11-Wednesday.assets/RMSProp1.png){:hight="75%" width="75%"}

&emsp;&emsp;也就意味着，如果$\alpha$趋近于0，相当于认为之前迭代的陡峭程度是不重要的，当前迭代的反而很重要。通过调整$\alpha$

![RMSProp2](/assets/blog_res/2022-05-11-Wednesday.assets/RMSProp2.png){:hight="75%" width="75%"}

&emsp;&emsp;那么同样地，将RMSProp与动量（Momentum）结合起来一起优化梯度下降。

&emsp;&emsp;当然这样复杂的实现在pytorch当中是有库的。

## 3、学习率优化三：可以应对越接近临界点越容易出现“喷发”的现象

![blow up](/assets/blog_res/2022-05-11-Wednesday.assets/blow%20up.png){:hight="75%" width="75%"}

&emsp;&emsp;应对这一现象，一般可以通过将学习率设计成随时间递减的函数。这一方法的出发点是认为随着时间的进行，通过不断地迭代和参数更新会离临界点越来越近，那么通过将其设计成随时间递减地函数可以有效的解决“喷发”现象。

&emsp;&emsp;其中一种解决方式是：Learning Rate Decay

![learning rate decay](/assets/blog_res/2022-05-11-Wednesday.assets/Learning%20Rate%20Decay.png)

&emsp;&emsp;另一种比较传奇的是：Warm Up

![warm up](/assets/blog_res/2022-05-11-Wednesday.assets/Warm%20Up.png)

&emsp;&emsp;Warm Up比较匪夷所思的地方在于：为什么要先变快呢？变快要变多快？变慢要变多慢？啥时候开始下降？

&emsp;&emsp;有一种可能的解释是：一开始我们需要计算$\sigma$，而这个$\sigma$呢其实是一个统计量，那么当一开始数据极其稀少的时候其是不太精准的，所以一开始让学习率较小是为了在初始的地方收集一些有关error surface等的数据并反复进行探索的，当$\sigma$相对比较精准以后再让学习率随着时间下降。

# （二）Optimization总结

![summary](/assets/blog_res/2022-05-11-Wednesday.assets/summary.png){:hight="75%" width="75%"}

&emsp;&emsp;尽管动量与$\sigma$都是过去所有梯度的组合，但是二者对于梯度的组织方式大相径庭，动量是考虑了梯度的方向，本质上是个向量，而均方差只考虑大小，所以二者不会抵消且最终得到的也是个向量。

# （三）更多关于Optimization的内容

[2020 Optimization扩展1](https://youtu.be/4pUmZ8hXlHM)

[2020 Optimization扩展2](https://youtu.be/e03YKGHXnL8)

以上是2020年的由助教进行讲解的，2022年有新的。

# （四）留待下堂课解决的问题

&emsp;&emsp;1、是否可以直接“神罗天征”夷平山峰

![blow](/assets/blog_res/2022-05-11-Wednesday.assets/blow.png){:hight="75%" width="75%"}

# 附录

[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)（Adam：RMSProp + Momentum）

![adam](/assets/blog_res/2022-05-11-Wednesday.assets/Adam.png){:hight="75%" width="75%"}

[On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265)（RAdam，Adam加上Warm up，如果想要了解更多关于Warm Up的话）

[Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)（是否可以“神罗天征”夷平山峰）