---
title: 李宏毅机器学习笔记-3-做好Optimizaiton之极值点（local minima/maxima）与鞍点（saddle point）
date: 2022-05-06 14:00:00 +0800
categories: [机器学习入门]
tags: [李宏毅机器学习课程笔记]
pin: false
author: 郑威琦

toc: true
comments: true
math: false
mermaid: true

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# （一）优化Optimization

## 1、为什么Optimization会失败

&emsp;&emsp;一般而言，在进行梯度下降的过程陷入到了临界点（critical point）会导致Optimization失败。临界点一般包括极小/大值点（local minima/maxima）和鞍点（saddle point）。

![optimization failed](/assets/blog_res/2022-05-06-Friday.assets/optimization%20failed.png){:hight="50%" width="50%"}

&emsp;&emsp;由于梯度下降是对损失函数（Loss Function）进行梯度下降，寻找能够使得损失函数最小化的一组参数（其可以归类为函数的最优化问题，类比遗传算法）,尽管损失函数不能很好地被表示（写）出来，但是我们可以利用泰勒展开来进行分类讨论，即“实际优化问题的目标函数往往比较复杂，为了使问题简化，通常将目标函数在某点附近展开为泰勒多项式来逼近原函数”。

&emsp;&emsp;在同济版高等数学中，第九章第九节有对二元函数的泰勒公式的讲解，以下插入对多元函数的泰勒公式的简单阐述。

## 2、多元函数的泰勒公式

### （1）二元函数的泰勒公式

&emsp;&emsp;**定理 设$z=f(x,y)$在点$(x,y)$的某一邻域内连续且有$(n+1)$阶连续偏导数，$(x_ 0 + h,y_ 0 + k)$为此邻域内任一点，则有**

![binary taylor](/assets/blog_res/2022-05-06-Friday.assets/binary%20taylor.png){: hight="50%" width="50%"}

![binary taylor2](/assets/blog_res/2022-05-06-Friday.assets/binary%20taylor2.png){: hight="50%" width="50%"}

### （2）黑塞矩阵

&emsp;&emsp;将二元函数的泰勒公式用矩阵的形式写出来，那么加和中的每一项可以写为$X^T H X$，其中H即为Hession Matrix，$X=(x_1,x_2,...,x_n)^T$。

&emsp;&emsp;二元函数的任意项的Hession Matrix如下：

![binary hession](/assets/blog_res/2022-05-06-Friday.assets/binary%20hession.png)

&emsp;&emsp;多元函数的任意项的Hession Matrix如下：

![multiple hession](/assets/blog_res/2022-05-06-Friday.assets/multiple%20hession.png)

## 3、如何判断当前是陷入到极小值、极大值或者是鞍点？

&emsp;&emsp;根据多元函数的泰勒展开有损失函数在$\theta^`$处展开的前三项：

$$L(\theta) \approx L(\theta^ `) + (\theta - \theta^ `)g + \frac{ 1 }{ 2 }( \theta - \theta^ `)H( \theta - \theta^ `)$$

&emsp;&emsp;其中，g为梯度，H为Hession Matrix。  

&emsp;&emsp;若当前处在临界点，则其一阶偏导（即梯度）为0，二阶偏导的状态可以确定处在哪个临界点。

&emsp;&emsp;则进行讨论有如下结论：

![judge critical point](/assets/blog_res/2022-05-06-Friday.assets/judge%20criticle.png){: hight="50%" width="50%"}

&emsp;&emsp;当黑塞矩阵为正定矩阵时，即其所有的特征值都大于零，临界点就为极小值点；当黑塞矩阵为负定矩阵时，即其所有的特征值都小于零，临界点就为极大值点；而当黑塞矩阵有时正有时负，即其特征值有正有负，临界点就为鞍点。

&emsp;&emsp;但是当我们将黑塞矩阵的维度由二维推广至多维时，即二元推广至多元，黑塞矩阵的维度越大，其为正定矩阵或负定矩阵的可能性就越小，一般其特征都是有正有负的，所以在实际的训练过程中，主要都使处理鞍点带来的Optimization不完全的情况。

## 4、如果陷入到鞍点如何解决？

&emsp;&emsp;那么具体如何应对梯度下降陷入到鞍点呢？我们可以从鞍点的特性得到启发，鞍点不是一个坑，它是可以延某一个方向继续下降的，所以我们只要找到它可以下降的方向下降它就好了。这还要用到黑塞矩阵。

&emsp;&emsp;对于损失函数的泰勒展开（前三项），第二项为0，则为：

$$L(\theta) \approx L(\theta^ `) + \frac{ 1 }{ 2 }( \theta - \theta^ `)H( \theta - \theta^ `)$$

&emsp;&emsp;对于$H$我们设其特征向量为$\nu$，设特征值为$\lambda$，则

$$H\nu = \lambda \nu$$

&emsp;&emsp;故

$$\nu^T H \nu = \nu^T(\lambda \nu) = \lambda ||\nu||^2$$

&emsp;&emsp;所以，当$\lambda<0$时，$\nu^T H \nu < 0$。若令$\theta - \theta^` = \nu$，则当$\lambda < 0$且$\theta = \theta^` + \nu$时，新的$\theta$就是比当前的点还低点。

## 5、Saddle Point v.s. Local Minima

&emsp;&emsp;在某维度下看是极值点，那么会不会在高维中看其实使鞍点。

![saddle local](/assets/blog_res/2022-05-06-Friday.assets/saddle%20lcoal.png){: hight="50%" width="50%"}