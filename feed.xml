<feed xmlns="http://www.w3.org/2005/Atom"> <id>/</id><title>Wecky Zheng</title><subtitle>这是通过Chirpy主题配置而成的</subtitle> <updated>2022-05-13T02:12:31+00:00</updated> <author> <name>Wecky Zheng</name> <uri>/</uri> </author><link rel="self" type="application/atom+xml" href="/feed.xml"/><link rel="alternate" type="text/html" hreflang="zh-CN" href="/"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2022 Wecky Zheng </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>李宏毅机器学习笔记-8-CNN</title><link href="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8/" rel="alternate" type="text/html" title="李宏毅机器学习笔记-8-CNN" /><published>2022-05-12T06:00:00+00:00</published> <updated>2022-05-12T06:00:00+00:00</updated> <id>/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8/</id> <content src="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8/" /> <author> <name>郑威琦</name> </author> <category term="机器学习入门" /> <summary> （一） </summary> </entry> <entry><title>李宏毅机器学习笔记-7-Training Tip 1：Batch Normalization</title><link href="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7/" rel="alternate" type="text/html" title="李宏毅机器学习笔记-7-Training Tip 1：Batch Normalization" /><published>2022-05-12T01:00:00+00:00</published> <updated>2022-05-12T07:14:36+00:00</updated> <id>/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7/</id> <content src="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7/" /> <author> <name>郑威琦</name> </author> <category term="机器学习入门" /> <summary> （一）Batch Normalization（批次标准化）   这个方法就是上节课（笔记5）所留下的如何能铲平山的一个方法。 1、Batch Normalization的最终目标   其最终达到的效果是将error surface处理地较为平整，能够更好地也更快地收敛。 2、Batch Normalization的具体过程 （1）在训练（training）当中   Batch Normalization其实就是对一个训练批次（batch）中所有的特征（feature）进行归一化/标准化（Normalization）。先是求得所有数据的均值，当然这个数据是个向量，对于不同数据中同一维度的数加起来除以batch size得到均值，然后再得到标准差，然后根据每一项数据减去相应的均值后，再除以标准差来作为归一化后的数据。如图：   输入层的话，由于... </summary> </entry> <entry><title>李宏毅机器学习笔记-6-分类（完全从技术的角度，不去追根溯源，只追求如何做到）</title><link href="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6/" rel="alternate" type="text/html" title="李宏毅机器学习笔记-6-分类（完全从技术的角度，不去追根溯源，只追求如何做到）" /><published>2022-05-11T06:00:00+00:00</published> <updated>2022-05-12T07:14:36+00:00</updated> <id>/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6/</id> <content src="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6/" /> <author> <name>郑威琦</name> </author> <category term="机器学习入门" /> <summary> （一）分类模型框架 1、利用独热编码（one-hot）处理输入输出 2、回归与分类的数学模型的对比 3、soft-max 4、分类常用loss function   如果选择均方差作为损失函数的话会出现下面的问题。 （二）分类需要再找时间去详细学习，这里面涉及到概率分布的一些推到   1、李宏毅再youtube上的课程   2、各种书籍（有时间可以买一下李沐的《动手学深度学习》第二版）   3、Logistic Regression </summary> </entry> <entry><title>李宏毅机器学习笔记-5-做好Optimization之学习率</title><link href="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5/" rel="alternate" type="text/html" title="李宏毅机器学习笔记-5-做好Optimization之学习率" /><published>2022-05-11T01:00:00+00:00</published> <updated>2022-05-11T08:21:38+00:00</updated> <id>/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5/</id> <content src="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5/" /> <author> <name>郑威琦</name> </author> <category term="机器学习入门" /> <summary> （一）学习率 1、学习率优化一：可以灵活应对不同的error surface   学习率这一超参数在训练的过程中也是发挥着举重若轻的作用，简单来说，学习率决定了根据梯度下降更新参数的时候参数下降的速率，或是每次更新的步伐大小。如果学习率偏大，在应对较陡的参数曲线时会引起震荡，但是在应对较为平缓的参数曲线时则可以发挥出其快速接近的作用。   而对于不同的参数，其对应的参数曲线不一致，所以对于不同的参数也要有不同的d学习率。我们希望在面对较为陡峭的error surface时，学习率可以小一点；在面对较为平缓的error surface时，学习率可以大一点。   那么对任意一个参数有： \[\theta^{t+1}_i \leftarrow \theta^{t}_i - \eta g^{t}_i\] \[g^{t}_i = \frac{\pa... </summary> </entry> <entry><title>李宏毅机器学习笔记-4-做好Optimization之批次训练（batch）和动量（momentum）</title><link href="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4/" rel="alternate" type="text/html" title="李宏毅机器学习笔记-4-做好Optimization之批次训练（batch）和动量（momentum）" /><published>2022-05-09T06:00:00+00:00</published> <updated>2022-05-11T08:17:17+00:00</updated> <id>/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4/</id> <content src="/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4/" /> <author> <name>郑威琦</name> </author> <category term="机器学习入门" /> <summary> （一）一些英文术语 1、batch（批次）   将一个训练集进行切分，每一块就叫一个batch，训练过程中只有输入一个batch中的所有数据才可以计算出损失函数进而更新参数。   个人理解：一个batch中的数据数相当于输入神经元的个数，即所构造的神经网络的输入层神经元个数。 2、update（更新）   一次参数更新。 3、epoch   对训练集切分出的所有块都训练过一次叫做一次epoch（机翻意思是时期、纪元）。 4、shuffle（洗牌）   由于对训练集可以做出不同形式的切分，在完成一次epoch后对训练集重新切分叫做shuffle，机翻就很形象了：洗牌。 （二）浅谈batch（也是一个超参数Parameter） 1、Small batch v.s. Large batch   Small batch的意思是切分出的batch数目小，Large ... </summary> </entry> </feed>
