[ { "title": "李宏毅机器学习笔记-8-CNN", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-12 06:00:00 +0000", "snippet": " （一）" }, { "title": "李宏毅机器学习笔记-7-Training Tip 1：Batch Normalization", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-12 01:00:00 +0000", "snippet": " （一）Batch Normalization（批次标准化）  这个方法就是上节课（笔记5）所留下的如何能铲平山的一个方法。1、Batch Normalization的最终目标  其最终达到的效果是将error surface处理地较为平整，能够更好地也更快地收敛。2、Batch Normalization的具体过程（1）在训练（training）当中  Batch Normalization其实就是对一个训练批次（batch）中所有的特征（feature）进行归一化/标准化（Normalization）。先是求得所有数据的均值，当然这个数据是个向量，对于不同数据中同一维度的数加起来除以batch size得到均值，然后再得到标准差，然后根据每一项数据减去相应的均值后，再除以标准差来作为归一化后的数据。如图：  输入层的话，由于没有激活函数，所以直接对原始数据进行处理就好了。那么当进入到第二层之后，对每一层的输入都要进行归一化，但由于第二层之后每一层都有激活函数，那么此时既可以再激活之前进行归一化，也可以再激活之后进行归一化。对于sigmoid来说在激活之前进行归一化会稍稍好一些。如图：  此时就是牵一发而动全身。（2）在测试（testing/inference）当中  在做实验的时候也许是有比较成块的测试集，这时候是可以将测试集也像训练集一样分成batch。但是在实际中，往往都是来一条数据就要输入到训练好的网络中来输出，所以此时的均值$\\mu$和标准差$\\sigma$该如何取？如下图：  在pytorch中会在训练的时候计算一个叫做动态平均（moving average）的数，他会动态的计算并更新$\\bar{\\mu}$，那么对于标准差而言也是一样的。其中$p$为一个超参数。然后把计算出来的均值和标准差用于测试时候的归一化。3、其他的归一化方法  当然归一化的方法不是唯一的，除了批次归一化还有很多的归一化方法。如图：附录【知乎】什么是批标准化 (Batch Normalization)【csdn】什么是批标准化 (Batch Normalization)  当然上面俩是野生的资料，更多的资料还可以通过论文、老师的youtube等深入了解。" }, { "title": "李宏毅机器学习笔记-6-分类（完全从技术的角度，不去追根溯源，只追求如何做到）", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-11 06:00:00 +0000", "snippet": "（一）分类模型框架1、利用独热编码（one-hot）处理输入输出2、回归与分类的数学模型的对比3、soft-max4、分类常用loss function  如果选择均方差作为损失函数的话会出现下面的问题。（二）分类需要再找时间去详细学习，这里面涉及到概率分布的一些推到  1、李宏毅再youtube上的课程  2、各种书籍（有时间可以买一下李沐的《动手学深度学习》第二版）  3、Logistic Regression" }, { "title": "李宏毅机器学习笔记-5-做好Optimization之学习率", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-11 01:00:00 +0000", "snippet": " （一）学习率1、学习率优化一：可以灵活应对不同的error surface  学习率这一超参数在训练的过程中也是发挥着举重若轻的作用，简单来说，学习率决定了根据梯度下降更新参数的时候参数下降的速率，或是每次更新的步伐大小。如果学习率偏大，在应对较陡的参数曲线时会引起震荡，但是在应对较为平缓的参数曲线时则可以发挥出其快速接近的作用。  而对于不同的参数，其对应的参数曲线不一致，所以对于不同的参数也要有不同的d学习率。我们希望在面对较为陡峭的error surface时，学习率可以小一点；在面对较为平缓的error surface时，学习率可以大一点。  那么对任意一个参数有：\\[\\theta^{t+1}_i \\leftarrow \\theta^{t}_i - \\eta g^{t}_i\\]\\[g^{t}_i = \\frac{\\partial L}{\\partial \\theta_i}|_{\\theta=\\theta_i}\\]  其中，t为迭代序号，i为参数序号，$\\eta$为学习率。在此基础上对参数更新进行优化：\\[\\theta^{t+1}_i \\leftarrow \\theta^{t}_i - \\frac{\\eta}{\\sigma^{t}_i}g^t_i\\]  其中增加$\\sigma$这一参数来根据error surface动态调整学习率。同时给出$\\sigma^t_i$：\\[\\sigma^t_i = \\sqrt{\\frac{1}{t+1}\\displaystyle \\sum^t_{i=0} (g^t_i)^2}\\]  以下给出具体迭代次数的$\\sigma$：  从图中可以看出，$\\sigma^t_i$就为过去所有更新的梯度平方的均值再开更号。  那么具体为何这么设计？这么设计又是怎样使得学习率动态调整呢？以下图为例：  如果$\\theta_1$对应的曲线较缓，那么上面每一次计算的梯度就较小，求得他们的均方根就比较小，每一次就会进行较大的调整。同理对应$\\theta_2$的曲线。2、学习率优化二：可以应对同一参数的error surface中的陡峭程度不同部分  为了能够应对同一参数曲线的不同陡峭部分，加入了新的参数$\\alpha(0&amp;lt;\\alpha&amp;lt;1)$来控制之前迭代的梯度与当前迭代的梯度的占比，即重要性。\\[\\theta^{t+1}_i \\leftarrow \\theta^{t}_i - \\frac{\\eta}{\\sigma^{t}_i}g^t_i\\]\\[\\sigma^t_i = \\sqrt{\\alpha(\\sigma^{t-1}_i)^2 + (1-\\alpha)(g^t_i)^2}\\]  也就意味着，如果$\\alpha$趋近于0，相当于认为之前迭代的陡峭程度是不重要的，当前迭代的反而很重要。通过调整$\\alpha$  那么同样地，将RMSProp与动量（Momentum）结合起来一起优化梯度下降。  当然这样复杂的实现在pytorch当中是有库的。3、学习率优化三：可以应对越接近临界点越容易出现“喷发”的现象  应对这一现象，一般可以通过将学习率设计成随时间递减的函数。这一方法的出发点是认为随着时间的进行，通过不断地迭代和参数更新会离临界点越来越近，那么通过将其设计成随时间递减地函数可以有效的解决“喷发”现象。  其中一种解决方式是：Learning Rate Decay  另一种比较传奇的是：Warm Up  Warm Up比较匪夷所思的地方在于：为什么要先变快呢？变快要变多快？变慢要变多慢？啥时候开始下降？  有一种可能的解释是：一开始我们需要计算$\\sigma$，而这个$\\sigma$呢其实是一个统计量，那么当一开始数据极其稀少的时候其是不太精准的，所以一开始让学习率较小是为了在初始的地方收集一些有关error surface等的数据并反复进行探索的，当$\\sigma$相对比较精准以后再让学习率随着时间下降。（二）Optimization总结  尽管动量与$\\sigma$都是过去所有梯度的组合，但是二者对于梯度的组织方式大相径庭，动量是考虑了梯度的方向，本质上是个向量，而均方差只考虑大小，所以二者不会抵消且最终得到的也是个向量。（三）更多关于Optimization的内容2020 Optimization扩展12020 Optimization扩展2以上是2020年的由助教进行讲解的，2022年有新的。（四）留待下堂课解决的问题  1、是否可以直接“神罗天征”夷平山峰附录Adam: A Method for Stochastic Optimization（Adam：RMSProp + Momentum）On the Variance of the Adaptive Learning Rate and Beyond（RAdam，Adam加上Warm up，如果想要了解更多关于Warm Up的话）Visualizing the Loss Landscape of Neural Nets（是否可以“神罗天征”夷平山峰）" }, { "title": "李宏毅机器学习笔记-4-做好Optimization之批次训练（batch）和动量（momentum）", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-09 06:00:00 +0000", "snippet": "（一）一些英文术语1、batch（批次）  将一个训练集进行切分，每一块就叫一个batch，训练过程中只有输入一个batch中的所有数据才可以计算出损失函数进而更新参数。  个人理解：一个batch中的数据数相当于输入神经元的个数，即所构造的神经网络的输入层神经元个数。2、update（更新）  一次参数更新。3、epoch  对训练集切分出的所有块都训练过一次叫做一次epoch（机翻意思是时期、纪元）。4、shuffle（洗牌）  由于对训练集可以做出不同形式的切分，在完成一次epoch后对训练集重新切分叫做shuffle，机翻就很形象了：洗牌。（二）浅谈batch（也是一个超参数Parameter）1、Small batch v.s. Large batch  Small batch的意思是切分出的batch数目小，Large batch的意思是切分出的batch数目大，比如说20条数据，每一条数据都分为一个batch。那么根据课程上所给出的实验数据可以看出，切分出来的batch数目越小，更新一次的时间会较长一些，但是更加powerful；而切分的batch数目越多，就越noisy。  但是结合更新一次和完成所有切分处理训练的时间来看，切分的batch数目不那么大，反而会更快。  然而batch数目适当多一些，其优势往往会比单纯追求训练速度要大很多。当batch数目相对较多时，其训练的准确率比较少的batch数目的要好很多，收敛地越好。究其原因在于，当batch增多时，前一个是critical point的，对应着下一个batch可能就不是critical point，这样就会继续下降，而不是卡在一个较高的点位。这也为什么说batch数目越多，越noisy，而越是“吵闹”，就越有可能跳出当前的临界点。  以下是二者的一个总结对比：2、好的Minima（Flat Minima）与坏的Minima（Sharp Minima）  有时候小的batch会造成过拟合，即在训练集上表现良好，但是在测试集上的表现却让人失望。而大的batch在一定程度上会减少这样的落差。  这个现象的产生来自于上面的Flat Minima与Sharp Minima。好的Minima比较平缓，坏的则是个陡峭的峡谷，假设测试集背后的分布恰好是训练集分布的平移，且恰好高峰与低谷相对应，那么此时在训练集表现很好的模型到了测试集上就奇差无比。为什么说这样的低谷不好呢？就在于此。你看Flat Minima，地势平缓，即便平移很多，测试集上的结果也与训练集上的相差不多。  那么当batch比较小的时候，noisy的程度不是那么高，就不容易跳出这个低谷，而只有batch相对较大时才能跳出。（三）动量（Momentum）  将梯度下降更新参数的过程很形象地描绘出来就是以下的样子：  那么被动量改进后的梯度下降在找到梯度后，不立即以负梯度进行更新，而是把新的负梯度与上一个行进的方向向量进行向量叠加，得出的这个叠加向量作为更新方向，这就让梯度下降具有了一定的“惯性”。  个人理解：这里应该是借用物理学当中的动量的概念，来给梯度下降进行优化的，使之也具有了动量，或者说惯性。这样的好处就是对于一些较为平缓的Minima也有机会跳出去，因为到了低谷的点并不会立即停止，而是依照其惯性继续照相应方向行进。（四）关于这节课程的疑问  1、batch在实际的神经网络中如何操作的？涉及到实际的操作和代码是怎样的？目前对于batch等的认知还比较抽象，需要在实践中进行完善。  2、batch作为超参数在实际过程中如何很好地进行调节？附录On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima" }, { "title": "李宏毅机器学习笔记-3-做好Optimizaiton之极值点（local minima/maxima）与鞍点（saddle point）", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-3/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-06 06:00:00 +0000", "snippet": " （一）优化Optimization1、为什么Optimization会失败  一般而言，在进行梯度下降的过程陷入到了临界点（critical point）会导致Optimization失败。临界点一般包括极小/大值点（local minima/maxima）和鞍点（saddle point）。  由于梯度下降是对损失函数（Loss Function）进行梯度下降，寻找能够使得损失函数最小化的一组参数（其可以归类为函数的最优化问题，类比遗传算法）,尽管损失函数不能很好地被表示（写）出来，但是我们可以利用泰勒展开来进行分类讨论，即“实际优化问题的目标函数往往比较复杂，为了使问题简化，通常将目标函数在某点附近展开为泰勒多项式来逼近原函数”。  在同济版高等数学中，第九章第九节有对二元函数的泰勒公式的讲解，以下插入对多元函数的泰勒公式的简单阐述。2、多元函数的泰勒公式（1）二元函数的泰勒公式  定理 设$z=f(x,y)$在点$(x,y)$的某一邻域内连续且有$(n+1)$阶连续偏导数，$(x_ 0 + h,y_ 0 + k)$为此邻域内任一点，则有（2）黑塞矩阵  将二元函数的泰勒公式用矩阵的形式写出来，那么加和中的每一项可以写为$X^T H X$，其中H即为Hession Matrix，$X=(x_1,x_2,…,x_n)^T$。  二元函数的任意项的Hession Matrix如下：  多元函数的任意项的Hession Matrix如下：3、如何判断当前是陷入到极小值、极大值或者是鞍点？  根据多元函数的泰勒展开有损失函数在$\\theta^`$处展开的前三项：\\[L(\\theta) \\approx L(\\theta^ `) + (\\theta - \\theta^ `)g + \\frac{ 1 }{ 2 }( \\theta - \\theta^ `)H( \\theta - \\theta^ `)\\]  其中，g为梯度，H为Hession Matrix。  若当前处在临界点，则其一阶偏导（即梯度）为0，二阶偏导的状态可以确定处在哪个临界点。  则进行讨论有如下结论：  当黑塞矩阵为正定矩阵时，即其所有的特征值都大于零，临界点就为极小值点；当黑塞矩阵为负定矩阵时，即其所有的特征值都小于零，临界点就为极大值点；而当黑塞矩阵有时正有时负，即其特征值有正有负，临界点就为鞍点。  但是当我们将黑塞矩阵的维度由二维推广至多维时，即二元推广至多元，黑塞矩阵的维度越大，其为正定矩阵或负定矩阵的可能性就越小，一般其特征都是有正有负的，所以在实际的训练过程中，主要都使处理鞍点带来的Optimization不完全的情况。4、如果陷入到鞍点如何解决？  那么具体如何应对梯度下降陷入到鞍点呢？我们可以从鞍点的特性得到启发，鞍点不是一个坑，它是可以延某一个方向继续下降的，所以我们只要找到它可以下降的方向下降它就好了。这还要用到黑塞矩阵。  对于损失函数的泰勒展开（前三项），第二项为0，则为：\\[L(\\theta) \\approx L(\\theta^ `) + \\frac{ 1 }{ 2 }( \\theta - \\theta^ `)H( \\theta - \\theta^ `)\\]  对于$H$我们设其特征向量为$\\nu$，设特征值为$\\lambda$，则\\[H\\nu = \\lambda \\nu\\]  故\\[\\nu^T H \\nu = \\nu^T(\\lambda \\nu) = \\lambda ||\\nu||^2\\]  所以，当$\\lambda&amp;lt;0$时，$\\nu^T H \\nu &amp;lt; 0$。若令$\\theta - \\theta^ = \\nu$，则当$\\lambda &amp;lt; 0$且$\\theta = \\theta^ + \\nu$时，新的$\\theta$就是比当前的点还低点。5、Saddle Point v.s. Local Minima  在某维度下看是极值点，那么会不会在高维中看其实使鞍点。" }, { "title": "李宏毅机器学习笔记-2-关于训练的指南", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-2/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-04 06:00:00 +0000", "snippet": "训练指南1、Loss on tranining data is large  在训练的过程当中，我们会输入training data来训练一开始我们设定的模型，当然，训练的过程本身就是迭代更新参数的过程。那么我们所训练的模型是否能够有很好的预测结果呢？这就涉及到对模型的优化和选择。当然，优化与选择也不是凭空进行的，而是对训练中看到的一些现象做出相应的分析和应对。  这里主要就给出李宏毅老师的训练指南，粗浅的给出一些问题的解决方案。如图：  如果训练的效果不尽如人意，首先要检查在训练集上的误差有多大，如果训练集上的误差都很大，那么说明目前的模型是有问题的，具体问题可以分成两种。第一种是模型的问题，目前基于源模型训练得到的集合中，没有可以使得误差较小的模型，此时就需要更换模型，加深模型的深度，像增加特征、增加神经网络层数以增加深度等等。那么第二种就是Optimizaiton做得不好，没有找到一组参数能够使得误差较小。针对这种问题可以去关注梯度下降算法、是否陷入到了局部最优等。  李宏毅老师在讲解这两种问题的时候给出的比喻十分生动也很准确。如果把模型训练比作大海捞针，那么前者是针不在你捞的海，后者则是你怎么捞也捞不到那根存在于海中的针。  那么如何区分这两种问题呢？当准备训练模型去解决一个新的问题时候，首先最好使用一些浅层的模型去训练，因为这些模型的Optimization已经做的比较充分。再使用深层的模型训练，若发现深层模型的误差比浅层的还要大，那么就是Optimization的问题。这一问题的解决在接下来的课程中会有讲解。2、Loss on training data is small（1）Loss on testing data is large   当训练集的误差较小，而测试集的误差较大时，相对比较容易解决的问题便是过拟合（overfitting），另外一个则是分布不匹配（mismatch）。①过拟合  直接先来一张图，这张图大概展示了overfitting的现象。  至于说深度较大、参数较多的模型为什么会产生这样的曲线，会在之后的课程当中从数学的角度去说明。  我个人理解过拟合的现象其实就是由于训练集的大小、多样性等与其深度、参数数量、特征维度等不相匹配所造成的，并不是单方向上的训练集少了或者是参数多了。  那么解决过拟合的最简单的方法就是增加训练资料，使模型被适当的限制住。增加训练资料最简单的就是根据原有的资料制作合乎情理的新资料。比如：  像CNN是一个比较有限制的模型，它是根据影像的特性进而限制，而全连接的神经网络其实是比较灵活的。②分布不匹配  举个最简单的例子，在概率论中我们学过均匀分布和正态分布，本来你基于训练集训练出来的是个正态分布，完了测试集是个均匀分布，那误差不大才怪呢。  是不是mismathc往往要看数据的产生方式，以及对问题的理解等等。具体的应对在作业10处会进行讲解。（2）Loss on testing data is small  这种训练完成了。3、如何选择/找到一个合适的模型  在这节课程中提到的方式是交叉验证（cross validation），将训练集分为Training Set和Validation Set来进行训练和验证。  进一步的有N-fold Cross Validation4、本节课遗留问题1、如何解决Optimization的问题2、在数学上，深度较大、参数较多的模型为什么会产生灵活多变的曲线3、如何判断误差较大是由于分布不匹配（mismatch）产生的，并且如何解决？" }, { "title": "李宏毅机器学习笔记-1-机器学习入门", "url": "/posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-1/", "categories": "机器学习入门", "tags": "李宏毅机器学习课程笔记", "date": "2022-05-02 06:00:00 +0000", "snippet": " （一） 什么是机器学习Machine Learning ≈ Looking for Function  （二） 如何使机器学习1、步骤一：找一个函数（Function with Unknown Parameters）  y = b + wx1——模型（Model），x——特征（Feature），w——权重（weight），b——偏置（bias）2、步骤二：定义损失函数（Define Loss from Training Data）  选择什么样的损失函数？是均值？亦或是均方差？3、步骤三：通过参数更新找一个使得当前损失函数值最小的模型（Optimization）（1）w*，b* = arg min(w，b) L（2）如何去更新参数呢？-&amp;gt; 梯度下降？（3）缺陷  提出一个问题：Local minima 与 Global minima，即局部最优与全局最优的问题，单纯的梯度下降容易陷入到局部最优。那如何解决呢？在后面的课程中有讲解。4、小结  总而言之，上述提到的三个步骤合起来就是训练（Trainging）。ML Framework  （三）对于模型的简单深化（纵向优化）1、增加特征（输入）个数  这里以课上的视频播放量为例，如果说wx1是将前1天的播放量作为输入预测下一天的，那么从表面上看显然不是很合理，那么就可以将前七天的播放量作为输入，于是就变为：\\[y = b + \\sum \\limits_ { i = i }^ 7 w_ j x_ j\\]  类似的，可以将前二十八天的播放量作为输入：\\[y = b + \\sum \\limits_ { i = i } ^ { 28 } w_ j x_ j\\]  同理，前五十六天：\\[y = b + \\sum \\limits_ { i = i } ^ { 56 } w_ j x_ j\\]  但是，从老师所给出的数据中可以看出：二十八天与五十六天的损失函数值已经相差无几，近乎没有进一步减小损失。2、深化为神经网络（基于数学的基础上借助生物学将其表达形象化后的形式）  （四）对于机器学习过程中寻找合适函数的剖析1、线性模型实在是太粗糙了，我们需要模拟得更加真切的模型！  2、假设有如下曲线需要机器学习  那么，可以将其拆分成y = constant和一些折线型的函数之和：  于是乎，既有了个可用的既定结论（已经在数学中被证明）：所有分段线性曲线均可被表示为常数线与一些折线的和  3、那对于连续可导的函数如何分解呢？  取有限的点并连线，取得点可少可多，越多就越逼近原曲线，可以一定程度上理解为微元法。4、如何用数学表示折线形函数呢？  可以利用带有参数的Sigmoid函数近似去表示。  那么对于新模型，则有如下表达式：\\[y = b + \\sum \\limits_ i c_ i Sigmoid(b_ i + w_ i x_ 1)\\]  即如图：  于是乎，对于y = b + $\\sum \\limits_j$wjxj，就有：\\[y = b + \\sum \\limits_ i c_ i Sigmoid(b_ i + \\sum \\limits_ j w_ {ij} x_ j)\\]5、小结  综上，借助生物学中神经网络的形象和概念，就可以将以上深化过程形象地描绘成以下：  那么借助于线性代数的知识可以将其进一步简化为：  同理：  则，其中需要更新的参数就有如下这些：  （五）Sigmoid –&amp;gt; Relu1、Relu（Rectified Linear Unit）可以更好模拟hard sigmoid  两个Relu叠起来就是一个hard sigmoid。\\[y = b + \\sum \\limits_ {2i} c_ i max(0, b_ i + \\sum \\limits_ j w_ {ij} x_ j)\\]2、那为什么Relu函数比较好呢？  后面的课程会讲到  （六）深度学习  经过上面的铺垫，那么就可以自然而然地抛出神经网络与深度网络和深度学习的概念与模型了。1、常见的深度学习模型  AlexNet(2012, 8 layers, los_rate: 16.4%), VGG(2014, 19 layers, los_rate: 7.3%), GoogleNet(2014, 22 layers, los_rate: 6.7%)2、“深”的意义何在呢？多层隐藏层实际上在哪里做出了怎样的贡献呢？  （七）本节课提出的待解决的问题1、全局最优与局部最优2、Relu函数为什么比Sigmoid好？3、多层隐藏层的作用" }, { "title": "组会分享（一）", "url": "/posts/%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB-%E4%B8%80/", "categories": "组会", "tags": "组会个人分享", "date": "2022-04-27 01:00:00 +0000", "snippet": "1 前沿科技视野拓展小蛮驴“沦陷”事件CVPR2022（IEEE国际计算机视觉与模式识别会议）机器之心量子位2 个人博客搭建1分钟免费博客搭建3 神经网络反向误差传播3.1 神经网络  神经网络实质是在生物学基础上（人类神经元）建立起来的数学模型。  多个神经元同向排列（纵或横）组成一层（layer）。3.2 感知机  感知机（Perceptron）由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层为M-P神经元。  感知机学习旨在求出将训练数据进行线性划分的分离超平面。  感知机的参数更新规则是基于梯度下降的，在这里不多作详述，直接给出西瓜书所给更新规则。3.3 基于标准梯度下降的误差逆传播算法  假设有网络N，其输入层有d个神经元，隐藏层有q个M-P神经元，输出层有l个神经元，且输入层与隐藏层、隐藏层与输出层为全连接。设输入层第i个神经元到隐藏层第h个神经元的权值为νih，隐藏层第h个神经元的输入为αh，阈值为γh，输出为bh，隐藏层第h个神经元到输出层第j个神经元的权值为ωhj，输出层第j个神经元的输入为βj，阈值为θj，输出为yj尖。  并且，令激活函数为Sigmoid函数。  由此，根据前向传播规则，我们可计算出这些参数的值：  然后，由于将网络在(Xk, Yk)上的均方误差视为损失函数，则将损失函数最小化就是我们参数更新的依据，故计算出目标的负梯度方向，并以此对参数进行调整。  均方误差如下：  根据梯度的数学定义，计算每个参数对应的负梯度。如下：  总而言之：  最后给出算法流程：4 附录4.1 感知机代码from math import *from Activation_Function import *# 解决线性可分的基于梯度下降的感知机class Perceptron: def __init__(self, input, output_true, learning_rate): self.x = input self.w = [0 for i in range(len(input[0]))] # 权值 self.y = [0 for i in range(len(input))] # y尖 self.y_true = output_true # y self.bias = 0 self.lr = learning_rate # 学习率 self.w_changes = [0 for i in range(len(input[0]))] # 前向传播计算出y值 def Forward_Propagation(self): for i in range(len(self.x)): for j in range(len(self.x[i])): self.y[i] = self.y[i] + self.x[i][j] * self.w[j] self.y[i] = self.y[i] + self.bias self.y[i] = Sgn(self.y[i]) # 基于梯度下降的权值更新，通过不断地极小化目标函数（欧式距离/均方差） def Weight_Update(self, i): for j in range(len(self.w)): self.w_changes[j] = self.lr * self.y_true[i] * self.x[i][j] self.w[j] = self.w[j] + self.w_changes[j] self.bias = self.bias + self.y_true[i] * self.lr # 判断是否正确完成线性分割 def whether_all_correct(self): for i in range(len(self.y)): if self.y[i] * self.y_true[i] &amp;lt;= 0: return i continue return -1def train(): input = [[1, 1], [1, 0], [0, 1], [0, 0]] output_true = [1, -1, -1, -1] # input = [[3, 3], [4, 3], [1, 1]] # output_true = [1, 1, -1] learning_rate = 0.5 p = Perceptron(input, output_true, learning_rate) p.Forward_Propagation() i = p.whether_all_correct() while i != -1: print(&quot;x: &quot; + str(p.x) + &quot; y: &quot; + str(p.y) + &quot; w:　&quot; + str(p.w) + &quot; b: &quot; + str(p.bias)) p.Weight_Update(i) p.Forward_Propagation() i = p.whether_all_correct() print(&quot;x: &quot; + str(p.x) + &quot; y: &quot; + str(p.y) + &quot; w:　&quot; + str(p.w) + &quot; b: &quot; + str(p.bias))4.2 BP代码（有问题，但可以帮助理解算法流程）from math import *from Activation_Function import *# 构建神经网络class NeuralNetwork: def __init__(self, input_neural_num, hidden_neural_num, output_neural_num, X, learning_rate): # 网络层数 self.input_neural_num = input_neural_num # 输入层神经元个数 self.hidden_neural_num = hidden_neural_num # 隐藏层神经元个数 self.output_neural_num = output_neural_num # 输出层神经元个数 # 输入 self.x = X # For instance(Page 105 in 西瓜书): X = [[0, 0, 1], [1, 0, 0], [0, 2, 0], [1, 1, 0], [2, 2, 0]] self.lr = learning_rate # 学习率 # 输出 self.niu_ih = [0 for i in range(input_neural_num * hidden_neural_num)] # input -&amp;gt; hidden self.gamma = [0 for i in range(hidden_neural_num)] # 隐藏层阈值向量 self.omega_hj = [0 for i in range(hidden_neural_num * output_neural_num)] # hidden -&amp;gt; output self.theta = [0 for i in range(output_neural_num)] # 输出层阈值向量 self.y = [0 for i in range(output_neural_num)] # 参数 self.b = [0 for i in range(hidden_neural_num)] # 梯度项 self.g = [0 for i in range(output_neural_num)] # 输出层梯度项 self.e = [0 for i in range(hidden_neural_num)] # 隐藏层梯度项 def ForwardPropagation(self, x): alpha = [] # 隐藏层输入 alpha_h = 0 # 计算隐藏层输出向量b for h in range(self.hidden_neural_num): for i in range(self.input_neural_num): alpha_h += x[i] * self.niu_ih[i * self.hidden_neural_num + h] alpha.append(alpha_h) alpha_h = 0 self.b[h] = Sigmoid(alpha[h] - self.gamma[h]) beta = [] # 输出层输入 beta_j = 0 # 计算输出层输出向量y for j in range(self.output_neural_num): for h in range(self.hidden_neural_num): beta_j += self.b[h] * self.omega_hj[h * self.output_neural_num + j] beta.append(beta_j) beta_j = 0 self.y[j] = Sigmoid(beta[j] - self.theta[j]) def Cal_g(self, x): for j in range(self.output_neural_num): self.g[j] = self.y[j] * (1 - self.y[j]) * (x[self.input_neural_num] - self.y[j]) def Cal_e(self): for h in range(self.hidden_neural_num): contribution = 0 for j in range(self.output_neural_num): contribution += self.g[j] * self.omega_hj[h * self.output_neural_num + j] self.e[h] = self.b[h] * (1 - self.b[h]) * contribution def BP(self, x): for h in range(self.hidden_neural_num): for j in range(self.output_neural_num): self.omega_hj[h * self.output_neural_num + j] += self.lr[1] * self.g[j] * self.b[h] for j in range(self.output_neural_num): self.theta[j] -= self.lr[1] * self.g[j] for i in range(self.input_neural_num): for h in range(self.hidden_neural_num): self.niu_ih[i * self.hidden_neural_num + h] += self.lr[0] * self.e[h] * x[i] for h in range(self.hidden_neural_num): self.gamma[h] -= self.lr[0] * self.e[h] # 迭代 def run(self): for x in self.x: self.ForwardPropagation(x) self.Cal_g(x) self.Cal_e() self.BP(x)if __name__ == &quot;__main__&quot;: input_neural_num = 2 hidden_neural_num = 2 output_neural_num = 2 X = [[0, 0, 1], [1, 0, 0], [1, 1, 0], [0, 2, 0], [2, 2, 0]] learning_rate = [0.6, 0.4] nn = NeuralNetwork(input_neural_num, hidden_neural_num, output_neural_num, X, learning_rate) for i in range(200): print(&quot;y: &quot; + str(nn.y)) nn.run()" }, { "title": "第一次分享规划", "url": "/posts/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%88%86%E4%BA%AB%E8%A7%84%E5%88%92/", "categories": "组会", "tags": "组会个人分享", "date": "2022-04-26 12:41:00 +0000", "snippet": "2022年4月27日第一次小组分享计划1、概述  主要分为三部分，总共耗时应不超过30分钟。这三部分分别为前沿科技视野拓展（目标检测（点云）在极端天气的应对之法）、个人本地文档建立、感知机（机器学习角度与神经网络角度）。2、前沿科技视野拓展  分享小蛮驴极端天气视频，对极端天气下的解决方案有一定的认知，并查阅相关资料和论文，对点云在极端天气下的识别有所了解，能够讲出一些方法和思路。3、个人本地文档建立  分享个人本地文档建立方法，分享B站up主的视频。4、从感知机到神经网络（从解决线性可分问题（最简单的为二分类问题）到解决线性不可分问题）  分享理论+实践。理论部分分享机器学习角度的感知机解决二分类问题和神经网络解决分线性不可分问题。实践就是用python所写的代码。  其中，感知机理论部分包括机器学习所提到的基于梯度下降的感知机学习策略。神经网络部分包含前向传播和基于标准梯度下降的反向误差传播。" } ]
